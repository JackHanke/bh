{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def1b67d",
   "metadata": {},
   "source": [
    "# Training Workbook for Supercomputer Environment\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c008e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup scripts...\n",
      "Starting single GPU training\n",
      "--- Training script running! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train batch 20 completed with loss 10411.2686 in 1.85s: 100%|██████████| 20/20 [01:22<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 6535.9121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation batch 2 completed with loss 7832.5693 in 2.20s.:  40%|████      | 2/5 [00:12<00:19,  6.47s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 15.12 MiB is free. Process 2073508 has 422.00 MiB memory in use. Process 1329639 has 2.40 GiB memory in use. Process 365641 has 534.00 MiB memory in use. Process 366918 has 598.00 MiB memory in use. Including non-PyTorch memory, this process has 27.25 GiB memory in use. Of the allocated memory 26.64 GiB is allocated by PyTorch, and 111.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/u1/j/jackh/bh/harm2d/pp.py:6383\u001b[39m\n\u001b[32m   6381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6382\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting single GPU training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m6383\u001b[39m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6385\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[32m   6386\u001b[39m \u001b[38;5;66;03m# save_path = os.environ['HOME']+f'/bh/movies/sc_frames/'\u001b[39;00m\n\u001b[32m   6387\u001b[39m \u001b[38;5;66;03m# plot_and_save_range(start=3000, end=3050, save_path=save_path)\u001b[39;00m\n\u001b[32m   6388\u001b[39m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/u1/j/jackh/bh/harm2d/pp.py:6179\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   6176\u001b[39m label_data = torch.cat(label_data, dim=\u001b[32m0\u001b[39m).to(device)\n\u001b[32m   6178\u001b[39m \u001b[38;5;66;03m# make prediction\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6179\u001b[39m pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6181\u001b[39m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[32m   6182\u001b[39m loss_value = loss_fn(pred, label_data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/global/u1/j/jackh/bh/harm2d/models/cnn/cnn.py:280\u001b[39m, in \u001b[36mCNN_DEPTH.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.bottleneck_layers(x)\n\u001b[32m    282\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.decoder(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:725\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/scenv/lib/python3.11/site-packages/torch/nn/modules/conv.py:720\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    710\u001b[39m         F.pad(\n\u001b[32m    711\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    719\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 39.38 GiB of which 15.12 MiB is free. Process 2073508 has 422.00 MiB memory in use. Process 1329639 has 2.40 GiB memory in use. Process 365641 has 534.00 MiB memory in use. Process 366918 has 598.00 MiB memory in use. Including non-PyTorch memory, this process has 27.25 GiB memory in use. Of the allocated memory 26.64 GiB is allocated by PyTorch, and 111.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 112.39053225517273\n",
      "Imports and setup done.\n"
     ]
    }
   ],
   "source": [
    "## Setup and configs\n",
    "# imports\n",
    "import os\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# global variables\n",
    "global notebook\n",
    "global axisym,set_cart,axisym,REF_1,REF_2,REF_3,set_cart,D,print_fieldlines\n",
    "global lowres1,lowres2,lowres3, RAD_M1, RESISTIVE, export_raytracing_GRTRANS, export_raytracing_RAZIEH,r1,r2,r3\n",
    "global r_min, r_max, theta_min, theta_max, phi_min,phi_max, do_griddata, do_box, check_files, kerr_schild\n",
    "\n",
    "notebook = 1\n",
    "\n",
    "# total data is shape (10000, 224, 48, 96)\n",
    "# harm_directory = '/global/u1/j/jackh/bh/harm2d'\n",
    "# harm_directory = '/global/homes/a/arjuna/bh/harm2d'\n",
    "harm_directory = os.environ['HOME']+f'/bh/harm2d'\n",
    "os.chdir(harm_directory)\n",
    "\n",
    "print(f'Running setup scripts...')\n",
    "start_time = time.time()\n",
    "%run -i setup.py build_ext --inplace\n",
    "%run -i pp.py build_ext --inplace\n",
    "print(f\"Execution time: {time.time() - start_time}\")\n",
    "\n",
    "# set params\n",
    "lowres1 = 1 # \n",
    "lowres2 = 1 # \n",
    "lowres3 = 1 # \n",
    "r_min, r_max = 1.0, 100.0\n",
    "theta_min, theta_max = 0.0, 9\n",
    "phi_min, phi_max = -1, 9\n",
    "do_box=0\n",
    "set_cart=0\n",
    "set_mpi(0)\n",
    "axisym=1\n",
    "print_fieldlines=0\n",
    "export_raytracing_GRTRANS=0\n",
    "export_raytracing_RAZIEH=0\n",
    "kerr_schild=0\n",
    "DISK_THICKNESS=0.03\n",
    "check_files=1\n",
    "notebook=1\n",
    "interpolate_var=0\n",
    "AMR = 0 # get all data in grid\n",
    "\n",
    "print('Imports and setup done.')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a22f0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55d6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 28 but got size 224 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m     label_data.append(data_tensor)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# final tensorize\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m batch_data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     88\u001b[39m label_data = torch.cat(label_data, dim=\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m## train model\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# make prediction\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 0. Expected size 28 but got size 224 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training utilities\n",
    "from utils.sc_utils import custom_batcher, tensorize_globals\n",
    "from models.cnn.cnn import CNN_3D\n",
    "\n",
    "# path to dumps\n",
    "dumps_path = '/pscratch/sd/l/lalakos/ml_data_rc300/reduced'\n",
    "os.chdir(dumps_path)\n",
    "# number of data points\n",
    "num_dumps = 11 - 1\n",
    "# batch size\n",
    "batch_size = 2\n",
    "# number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# access device, cuda device if accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set model\n",
    "model = CNN_3D().to(device)\n",
    "# set loss\n",
    "optim = torch.optim.Adam(params=model.parameters())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# get indexes for training data\n",
    "train_indexes, validation_indexes = custom_batcher(\n",
    "    batch_size=batch_size,\n",
    "    num_dumps=num_dumps,\n",
    "    split = 0.8,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "num_train_batches = len(train_indexes)//batch_size\n",
    "num_valid_batches = len(validation_indexes)//batch_size\n",
    "\n",
    "best_validation = float('inf')\n",
    "\n",
    "rgdump_griddata(dumps_path)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ## Training\n",
    "    model.train()\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    # shuffle training indexes\n",
    "    np.random.shuffle(train_indexes)\n",
    "\n",
    "    # list of average train/validation losses after each epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_indexes.reshape(-1, batch_size)), total=num_train_batches)\n",
    "    for batch_num, batch_indexes in prog_bar:\n",
    "        ## fetch and tensorize data\n",
    "        # NOTE everything is a global variable so it has to be this way. im sorry\n",
    "        batch_data, label_data = [], []\n",
    "        # batch_idx is the dump number\n",
    "        for batch_idx in batch_indexes:\n",
    "\n",
    "            # at every batch of size batch_size, we need to read in 2 * batch_size dumps\n",
    "            \n",
    "            ## get data frame\n",
    "            # get data into global context NOTE this is really slow\n",
    "            rblock_new(batch_idx)\n",
    "            rpar_new(batch_idx)\n",
    "            rdump_griddata(dumps_path, batch_idx)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            batch_data.append(data_tensor)\n",
    "\n",
    "            ## get label frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx+1)\n",
    "            rpar_new(batch_idx+1)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx+1)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            label_data.append(data_tensor)\n",
    "\n",
    "        # final tensorize\n",
    "        batch_data = torch.cat(batch_data, dim=0).to(device)\n",
    "        label_data = torch.cat(label_data, dim=0).to(device)\n",
    "\n",
    "        ## train model\n",
    "        # make prediction\n",
    "        pred = model.forward(batch_data)\n",
    "        # compute loss\n",
    "        loss_value = loss_fn(pred, label_data)\n",
    "        epoch_train_loss.append(loss_value)\n",
    "        # backprop\n",
    "        loss_value.backward()\n",
    "        # update paramts\n",
    "        optim.step()\n",
    "\n",
    "        prog_bar.set_description(f'Train batch {batch_num+1} completed with loss {loss_value.item():.4f}')\n",
    "\n",
    "    # training loss tracking\n",
    "    avg_loss_after_epoch = sum(epoch_train_loss)/len(epoch_train_loss)\n",
    "    train_losses.append(avg_loss_after_epoch)\n",
    "    print(f\"Train loss value: {avg_loss_after_epoch}\")\n",
    "\n",
    "\n",
    "    ## Validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "\n",
    "    prog_bar = tqdm(enumerate(validation_indexes.reshape(-1, batch_size)), total=num_valid_batches)\n",
    "    for batch_num, batch_indexes in prog_bar:\n",
    "        ## fetch and tensorize data\n",
    "        # NOTE everything is a global variable so it has to be this way. im sorry\n",
    "        batch_data, label_data = [], []\n",
    "        # batch_idx is the dump number\n",
    "        for batch_idx in batch_indexes:\n",
    "            ## get data frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx)\n",
    "            rpar_new(batch_idx)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            batch_data.append(data_tensor)\n",
    "\n",
    "            ## get label frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx+1)\n",
    "            rpar_new(batch_idx+1)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx+1)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            label_data.append(data_tensor)\n",
    "\n",
    "        # final tensorize\n",
    "        batch_data = torch.cat(batch_data, dim=0).to(device)\n",
    "        label_data = torch.cat(label_data, dim=0).to(device)\n",
    "\n",
    "        # make prediction\n",
    "        pred = model.forward(batch_data)\n",
    "\n",
    "        # compute loss\n",
    "        loss_value = loss_fn(pred, label_data)\n",
    "        epoch_valid_loss.append(loss_value)\n",
    "        \n",
    "        prog_bar.set_description(f'Validation batch {batch_num+1} completed with loss {loss_value.item():.4f}.')\n",
    "        \n",
    "    avg_vloss_after_epoch = sum(epoch_train_loss)/len(epoch_train_loss)\n",
    "    valid_losses.append(avg_vloss_after_epoch)\n",
    "    print(f\"Valid loss value: {avg_loss_after_epoch}\")\n",
    "\n",
    "    # checkpointing\n",
    "    if avg_vloss_after_epoch < best_validation:\n",
    "        best_validation = avg_vloss_after_epoch\n",
    "        save_path = os.environ['HOME'] + '/bh/' + model.save_path\n",
    "        model.save(save_path=save_path)\n",
    "\n",
    "# plot learning\n",
    "plt.plot([i for i in range(len(train_losses))], [loss.item() for loss in train_losses], label='Train Loss')\n",
    "# plt.plot([i for i in range(len(train_losses))], [avg_baseline_loss for _ in range(len(train_losses))], label='Predicting Avg Loss', linestyle='dashed')\n",
    "plt.plot([i for i in range(len(valid_losses))], [loss.item() for loss in valid_losses], label='Validation Loss')\n",
    "plt.title(f'Training and Validation Curve')\n",
    "plt.xlabel(f'Number of Batches')\n",
    "plt.ylabel(f'Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcbafe",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6754855-104f-4747-ba1d-6709eafef009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load in model\n",
    "import os\n",
    "import torch\n",
    "from models.cnn.cnn import CNN_3D\n",
    "\n",
    "# access device, cuda device if accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = os.environ['HOME'] + '/bh/models/cnn/saves/3dcnn_v0.0.0.pth'\n",
    "model = CNN_3D()\n",
    "model.load_state_dict(torch.load(f=model_path))\n",
    "model = model.to(device)\n",
    "print(f'Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a256c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'animate_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     12\u001b[0m predictions, latents \u001b[38;5;241m=\u001b[39m make_prediciton_frames(\n\u001b[1;32m     13\u001b[0m     net\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     14\u001b[0m     first_frame\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# animate\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43manimate_preds\u001b[49m(\n\u001b[1;32m     22\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions, \n\u001b[1;32m     23\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./movies/preds_movie.gif\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m     cb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'animate_preds' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.anim import make_prediciton_frames\n",
    "\n",
    "os.chdir(os.environ['HOME'] + '/bh/')\n",
    "\n",
    "# access device, cuda device if accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_path = os.environ['HOME']+'/bh/data.pkl'\n",
    "data = torch.load(f=save_path)\n",
    "# print(data.shape)\n",
    "\n",
    "predictions, latents = make_prediciton_frames(\n",
    "    net=model,\n",
    "    first_frame=data[0].unsqueeze(0), \n",
    "    make_latents=False,\n",
    "    num_frames=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# animate\n",
    "# animate_preds(\n",
    "#     predictions = predictions, \n",
    "#     save_path = './movies/preds_movie.gif',\n",
    "#     cb = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2578c42-a0f7-409f-b666-727a4047a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scenvkernel",
   "language": "python",
   "name": "scenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
