{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def1b67d",
   "metadata": {},
   "source": [
    "# Training Workbook for Supercomputer Environment\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c008e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup scripts...\n",
      "--- Training script running! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train batch 4 completed with loss 0.0453 in 0.69s: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.0604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation batch 1 completed with loss 586.4362 in 0.69s.: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 valid loss value: 0.0604\n",
      "Saved model as /global/homes/a/arjuna/bh/harm2d/models/cnn/saves/cnn_depth_v0.0.0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train batch 4 completed with loss 0.2012 in 0.79s: 100%|██████████| 4/4 [00:05<00:00,  1.28s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 151.1488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation batch 1 completed with loss 0.0803 in 0.69s.: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 valid loss value: 151.1488\n",
      "Imports and setup done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Setup and configs\n",
    "# imports\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# global variables\n",
    "global notebook\n",
    "global axisym,set_cart,axisym,REF_1,REF_2,REF_3,set_cart,D,print_fieldlines\n",
    "global lowres1,lowres2,lowres3, RAD_M1, RESISTIVE, export_raytracing_GRTRANS, export_raytracing_RAZIEH,r1,r2,r3\n",
    "global r_min, r_max, theta_min, theta_max, phi_min,phi_max, do_griddata, do_box, check_files, kerr_schild\n",
    "\n",
    "notebook = 1\n",
    "\n",
    "# total data is shape (10000, 224, 48, 96)\n",
    "# harm_directory = '/global/u1/j/jackh/bh/harm2d'\n",
    "harm_directory = '/global/homes/a/arjuna/bh/harm2d'\n",
    "os.chdir(harm_directory)\n",
    "\n",
    "print(f'Running setup scripts...')\n",
    "%run -i setup.py build_ext --inplace\n",
    "%run -i pp.py build_ext --inplace\n",
    "\n",
    "# set params\n",
    "lowres1 = 1 # \n",
    "lowres2 = 1 # \n",
    "lowres3 = 1 # \n",
    "r_min, r_max = 1.0, 100.0\n",
    "theta_min, theta_max = 0.0, 9\n",
    "phi_min, phi_max = -1, 9\n",
    "do_box=0\n",
    "set_cart=0\n",
    "set_mpi(0)\n",
    "axisym=1\n",
    "print_fieldlines=0\n",
    "export_raytracing_GRTRANS=0\n",
    "export_raytracing_RAZIEH=0\n",
    "kerr_schild=0\n",
    "DISK_THICKNESS=0.03\n",
    "check_files=1\n",
    "notebook=1\n",
    "interpolate_var=0\n",
    "AMR = 0 # get all data in grid\n",
    "\n",
    "print('Imports and setup done.')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a22f0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a55d6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 28 but got size 224 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     84\u001b[39m     label_data.append(data_tensor)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# final tensorize\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m batch_data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     88\u001b[39m label_data = torch.cat(label_data, dim=\u001b[32m0\u001b[39m).to(device)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m## train model\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# make prediction\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 0. Expected size 28 but got size 224 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training utilities\n",
    "from utils.sc_utils import custom_batcher, tensorize_globals\n",
    "from models.cnn.cnn import CNN_3D\n",
    "\n",
    "# path to dumps\n",
    "dumps_path = '/pscratch/sd/l/lalakos/ml_data_rc300/reduced'\n",
    "os.chdir(dumps_path)\n",
    "# number of data points\n",
    "num_dumps = 11 - 1\n",
    "# batch size\n",
    "batch_size = 2\n",
    "# number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# access device, cuda device if accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set model\n",
    "model = CNN_3D().to(device)\n",
    "# set loss\n",
    "optim = torch.optim.Adam(params=model.parameters())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# get indexes for training data\n",
    "train_indexes, validation_indexes = custom_batcher(\n",
    "    batch_size=batch_size,\n",
    "    num_dumps=num_dumps,\n",
    "    split = 0.8,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "num_train_batches = len(train_indexes)//batch_size\n",
    "num_valid_batches = len(validation_indexes)//batch_size\n",
    "\n",
    "best_validation = float('inf')\n",
    "\n",
    "rgdump_griddata(dumps_path)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ## Training\n",
    "    model.train()\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    # shuffle training indexes\n",
    "    np.random.shuffle(train_indexes)\n",
    "\n",
    "    # list of average train/validation losses after each epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_indexes.reshape(-1, batch_size)), total=num_train_batches)\n",
    "    for batch_num, batch_indexes in prog_bar:\n",
    "        ## fetch and tensorize data\n",
    "        # NOTE everything is a global variable so it has to be this way. im sorry\n",
    "        batch_data, label_data = [], []\n",
    "        # batch_idx is the dump number\n",
    "        for batch_idx in batch_indexes:\n",
    "\n",
    "            # at every batch of size batch_size, we need to read in 2 * batch_size dumps\n",
    "            \n",
    "            ## get data frame\n",
    "            # get data into global context NOTE this is really slow\n",
    "            rblock_new(batch_idx)\n",
    "            rpar_new(batch_idx)\n",
    "            rdump_griddata(dumps_path, batch_idx)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            batch_data.append(data_tensor)\n",
    "\n",
    "            ## get label frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx+1)\n",
    "            rpar_new(batch_idx+1)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx+1)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            label_data.append(data_tensor)\n",
    "\n",
    "        # final tensorize\n",
    "        batch_data = torch.cat(batch_data, dim=0).to(device)\n",
    "        label_data = torch.cat(label_data, dim=0).to(device)\n",
    "\n",
    "        ## train model\n",
    "        # make prediction\n",
    "        pred = model.forward(batch_data)\n",
    "        # compute loss\n",
    "        loss_value = loss_fn(pred, label_data)\n",
    "        epoch_train_loss.append(loss_value)\n",
    "        # backprop\n",
    "        loss_value.backward()\n",
    "        # update paramts\n",
    "        optim.step()\n",
    "\n",
    "        prog_bar.set_description(f'Train batch {batch_num+1} completed with loss {loss_value.item():.4f}')\n",
    "\n",
    "    # training loss tracking\n",
    "    avg_loss_after_epoch = sum(epoch_train_loss)/len(epoch_train_loss)\n",
    "    train_losses.append(avg_loss_after_epoch)\n",
    "    print(f\"Train loss value: {avg_loss_after_epoch}\")\n",
    "\n",
    "\n",
    "    ## Validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "\n",
    "    prog_bar = tqdm(enumerate(validation_indexes.reshape(-1, batch_size)), total=num_valid_batches)\n",
    "    for batch_num, batch_indexes in prog_bar:\n",
    "        ## fetch and tensorize data\n",
    "        # NOTE everything is a global variable so it has to be this way. im sorry\n",
    "        batch_data, label_data = [], []\n",
    "        # batch_idx is the dump number\n",
    "        for batch_idx in batch_indexes:\n",
    "            ## get data frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx)\n",
    "            rpar_new(batch_idx)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            batch_data.append(data_tensor)\n",
    "\n",
    "            ## get label frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx+1)\n",
    "            rpar_new(batch_idx+1)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx+1)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            label_data.append(data_tensor)\n",
    "\n",
    "        # final tensorize\n",
    "        batch_data = torch.cat(batch_data, dim=0).to(device)\n",
    "        label_data = torch.cat(label_data, dim=0).to(device)\n",
    "\n",
    "        # make prediction\n",
    "        pred = model.forward(batch_data)\n",
    "\n",
    "        # compute loss\n",
    "        loss_value = loss_fn(pred, label_data)\n",
    "        epoch_valid_loss.append(loss_value)\n",
    "        \n",
    "        prog_bar.set_description(f'Validation batch {batch_num+1} completed with loss {loss_value.item():.4f}.')\n",
    "        \n",
    "    avg_vloss_after_epoch = sum(epoch_train_loss)/len(epoch_train_loss)\n",
    "    valid_losses.append(avg_vloss_after_epoch)\n",
    "    print(f\"Valid loss value: {avg_loss_after_epoch}\")\n",
    "\n",
    "    # checkpointing\n",
    "    if avg_vloss_after_epoch < best_validation:\n",
    "        best_validation = avg_vloss_after_epoch\n",
    "        save_path = os.environ['HOME'] + '/bh/' + model.save_path\n",
    "        model.save(save_path=save_path)\n",
    "\n",
    "# plot learning\n",
    "plt.plot([i for i in range(len(train_losses))], [loss.item() for loss in train_losses], label='Train Loss')\n",
    "# plt.plot([i for i in range(len(train_losses))], [avg_baseline_loss for _ in range(len(train_losses))], label='Predicting Avg Loss', linestyle='dashed')\n",
    "plt.plot([i for i in range(len(valid_losses))], [loss.item() for loss in valid_losses], label='Validation Loss')\n",
    "plt.title(f'Training and Validation Curve')\n",
    "plt.xlabel(f'Number of Batches')\n",
    "plt.ylabel(f'Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcbafe",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6754855-104f-4747-ba1d-6709eafef009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "# load in model\n",
    "import os\n",
    "import torch\n",
    "from models.cnn.cnn import CNN_3D\n",
    "\n",
    "# access device, cuda device if accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_path = os.environ['HOME'] + '/bh/models/cnn/saves/3dcnn_v0.0.0.pth'\n",
    "model = CNN_3D()\n",
    "model.load_state_dict(torch.load(f=model_path))\n",
    "model = model.to(device)\n",
    "print(f'Model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a256c43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'animate_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     12\u001b[0m predictions, latents \u001b[38;5;241m=\u001b[39m make_prediciton_frames(\n\u001b[1;32m     13\u001b[0m     net\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     14\u001b[0m     first_frame\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# animate\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43manimate_preds\u001b[49m(\n\u001b[1;32m     22\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions, \n\u001b[1;32m     23\u001b[0m     save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./movies/preds_movie.gif\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     24\u001b[0m     cb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'animate_preds' is not defined"
     ]
    }
   ],
   "source": [
    "from utils.anim import make_prediciton_frames\n",
    "\n",
    "os.chdir(os.environ['HOME'] + '/bh/')\n",
    "\n",
    "# access device, cuda device if accessible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "save_path = os.environ['HOME']+'/bh/data.pkl'\n",
    "data = torch.load(f=save_path)\n",
    "# print(data.shape)\n",
    "\n",
    "predictions, latents = make_prediciton_frames(\n",
    "    net=model,\n",
    "    first_frame=data[0].unsqueeze(0), \n",
    "    make_latents=False,\n",
    "    num_frames=10,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# animate\n",
    "# animate_preds(\n",
    "#     predictions = predictions, \n",
    "#     save_path = './movies/preds_movie.gif',\n",
    "#     cb = True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2578c42-a0f7-409f-b666-727a4047a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scenvkernel",
   "language": "python",
   "name": "scenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
