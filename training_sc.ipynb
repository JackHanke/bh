{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def1b67d",
   "metadata": {},
   "source": [
    "# Training Workbook for Supercomputer Environment\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c008e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running setup scripts...\n",
      "Imports and setup done.\n"
     ]
    }
   ],
   "source": [
    "## Setup and configs\n",
    "# imports\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# global variables\n",
    "global notebook\n",
    "global axisym,set_cart,axisym,REF_1,REF_2,REF_3,set_cart,D,print_fieldlines\n",
    "global lowres1,lowres2,lowres3, RAD_M1, RESISTIVE, export_raytracing_GRTRANS, export_raytracing_RAZIEH,r1,r2,r3\n",
    "global r_min, r_max, theta_min, theta_max, phi_min,phi_max, do_griddata, do_box, check_files, kerr_schild\n",
    "\n",
    "notebook = 1\n",
    "\n",
    "# total data is shape (10000, 224, 48, 96)\n",
    "harm_directory = '/global/u1/j/jackh/bh/harm2d'\n",
    "os.chdir(harm_directory)\n",
    "\n",
    "print(f'Running setup scripts...')\n",
    "%run -i setup.py build_ext --inplace\n",
    "%run -i pp.py build_ext --inplace\n",
    "\n",
    "# set params\n",
    "lowres1 = 1 # \n",
    "lowres2 = 1 # \n",
    "lowres3 = 1 # \n",
    "r_min, r_max = 1.0, 100.0\n",
    "theta_min, theta_max = 0.0, 9\n",
    "phi_min, phi_max = -1, 9\n",
    "do_box=0\n",
    "set_cart=0\n",
    "set_mpi(0)\n",
    "axisym=1\n",
    "print_fieldlines=0\n",
    "export_raytracing_GRTRANS=0\n",
    "export_raytracing_RAZIEH=0\n",
    "kerr_schild=0\n",
    "DISK_THICKNESS=0.03\n",
    "check_files=1\n",
    "notebook=1\n",
    "interpolate_var=0\n",
    "AMR = 0 # get all data in grid\n",
    "\n",
    "print('Imports and setup done.')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a22f0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch 4 completed with loss 0.0648: 100%|██████████| 4/4 [01:05<00:00, 16.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss value: 0.06507378816604614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batch 1 completed with loss 0.0645.: 100%|██████████| 1/1 [00:15<00:00, 15.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss value: 0.06507378816604614\n",
      "Saved model as /global/homes/j/jackh/bh/models/cnn/saves/3dcnn_v0.0.0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# training utilities\n",
    "from utils.sc_utils import custom_batcher, tensorize_globals\n",
    "from models.cnn.cnn import CNN_3D\n",
    "\n",
    "# path to dumps\n",
    "dumps_path = '/pscratch/sd/l/lalakos/ml_data_rc300/reduced'\n",
    "os.chdir(dumps_path)\n",
    "# number of data points\n",
    "num_dumps = 11 - 1\n",
    "# batch size\n",
    "batch_size = 2\n",
    "# number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# access device, cuda device if acces8:30 is finesible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set model\n",
    "model = CNN_3D().to(device)\n",
    "# set loss\n",
    "optim = torch.optim.Adam(params=model.parameters())\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# get indexes for training data\n",
    "train_indexes, validation_indexes = custom_batcher(\n",
    "    batch_size=batch_size,\n",
    "    num_dumps=num_dumps,\n",
    "    split = 0.8,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "num_train_batches = len(train_indexes)//batch_size\n",
    "num_valid_batches = len(validation_indexes)//batch_size\n",
    "\n",
    "best_validation = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ## Training\n",
    "    model.train()\n",
    "    epoch_train_loss = []\n",
    "\n",
    "    # shuffle training indexes\n",
    "    np.random.shuffle(train_indexes)\n",
    "\n",
    "    # list of average train/validation losses after each epoch\n",
    "    train_losses, valid_losses = [], []\n",
    "\n",
    "    prog_bar = tqdm(enumerate(train_indexes.reshape(-1, batch_size)), total=num_train_batches)\n",
    "    for batch_num, batch_indexes in prog_bar:\n",
    "        ## fetch and tensorize data\n",
    "        # NOTE everything is a global variable so it has to be this way. im sorry\n",
    "        batch_data, label_data = [], []\n",
    "        # batch_idx is the dump number\n",
    "        for batch_idx in batch_indexes:\n",
    "            ## get data frame\n",
    "            # get data into global context NOTE this is really slow\n",
    "            rblock_new(batch_idx)\n",
    "            rpar_new(batch_idx)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            batch_data.append(data_tensor)\n",
    "\n",
    "            ## get label frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx+1)\n",
    "            rpar_new(batch_idx+1)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx+1)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            label_data.append(data_tensor)\n",
    "\n",
    "        # final tensorize\n",
    "        batch_data = torch.cat(batch_data, dim=0).to(device)\n",
    "        label_data = torch.cat(label_data, dim=0).to(device)\n",
    "\n",
    "        ## train model\n",
    "        # make prediction\n",
    "        pred = model.forward(batch_data)\n",
    "        # compute loss\n",
    "        loss_value = loss_fn(pred, label_data)\n",
    "        epoch_train_loss.append(loss_value)\n",
    "        # backprop\n",
    "        loss_value.backward()\n",
    "        # update paramts\n",
    "        optim.step()\n",
    "\n",
    "        prog_bar.set_description(f'Train batch {batch_num+1} completed with loss {loss_value.item():.4f}')\n",
    "\n",
    "    # training loss tracking\n",
    "    avg_loss_after_epoch = sum(epoch_train_loss)/len(epoch_train_loss)\n",
    "    train_losses.append(avg_loss_after_epoch)\n",
    "    print(f\"Train loss value: {avg_loss_after_epoch}\")\n",
    "\n",
    "\n",
    "    ## Validation\n",
    "    model.eval()\n",
    "    epoch_valid_loss = []\n",
    "\n",
    "    prog_bar = tqdm(enumerate(validation_indexes.reshape(-1, batch_size)), total=num_valid_batches)\n",
    "    for batch_num, batch_indexes in prog_bar:\n",
    "        ## fetch and tensorize data\n",
    "        # NOTE everything is a global variable so it has to be this way. im sorry\n",
    "        batch_data, label_data = [], []\n",
    "        # batch_idx is the dump number\n",
    "        for batch_idx in batch_indexes:\n",
    "            ## get data frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx)\n",
    "            rpar_new(batch_idx)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            batch_data.append(data_tensor)\n",
    "\n",
    "            ## get label frame\n",
    "            # get data into global context\n",
    "            rblock_new(batch_idx+1)\n",
    "            rpar_new(batch_idx+1)\n",
    "            rgdump_griddata(dumps_path)\n",
    "            rdump_griddata(dumps_path, batch_idx+1)\n",
    "            # format data as tensor\n",
    "            data_tensor = tensorize_globals(rho=rho, ug=ug, uu=uu, B=B)\n",
    "            # add to batch\n",
    "            label_data.append(data_tensor)\n",
    "\n",
    "        # final tensorize\n",
    "        batch_data = torch.cat(batch_data, dim=0).to(device)\n",
    "        label_data = torch.cat(label_data, dim=0).to(device)\n",
    "\n",
    "        # make prediction\n",
    "        pred = model.forward(batch_data)\n",
    "\n",
    "        # compute loss\n",
    "        loss_value = loss_fn(pred, label_data)\n",
    "        epoch_valid_loss.append(loss_value)\n",
    "        \n",
    "        prog_bar.set_description(f'Validation batch {batch_num+1} completed with loss {loss_value.item():.4f}.')\n",
    "        \n",
    "    avg_vloss_after_epoch = sum(epoch_train_loss)/len(epoch_train_loss)\n",
    "    valid_losses.append(avg_vloss_after_epoch)\n",
    "    print(f\"Valid loss value: {avg_loss_after_epoch}\")\n",
    "\n",
    "\n",
    "    # checkpointing\n",
    "    if avg_vloss_after_epoch < best_validation:\n",
    "        best_validation = avg_vloss_after_epoch\n",
    "        save_path = os.environ['HOME']+'/bh/' + model.save_path\n",
    "        model.save(save_path=save_path)\n",
    "\n",
    "# plot learning\n",
    "plt.plot([i for i in range(len(train_losses))], [loss.item() for loss in train_losses], label='Train Loss')\n",
    "# plt.plot([i for i in range(len(train_losses))], [avg_baseline_loss for _ in range(len(train_losses))], label='Predicting Avg Loss', linestyle='dashed')\n",
    "plt.plot([i for i in range(len(valid_losses))], [loss.item() for loss in valid_losses], label='Validation Loss')\n",
    "plt.title(f'Training and Validation Curve')\n",
    "plt.xlabel(f'Number of Batches')\n",
    "plt.ylabel(f'Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afcbafe",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a256c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# holy fuck TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
